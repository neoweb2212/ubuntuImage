sudo apt update
pip3 install scrapy
sudo apt update
pip3 install scrapy
sudo apt pip3 install scrapy
git clone git@github.com:neoweb2212/EmailFinder.git /home/ubuntu/EmailFinder
pip3 install uwsgi
sudo apt update
pip3 install scrapy
sudo apt pip3 install scrapy
sudo apt update
'apt list --upgradable
pip3 --version

pip install uwsgi
nano /home/ubuntu/EmailFinder/uwsgi.ini
sudo apt update
sudo apt install python3-pip -y
pip3 --version
pip3 install scrapy
sudo apt install python3-venv -y
python3 -m venv /home/ubuntu/EmailFinder/venv
source /home/ubuntu/EmailFinder/venv/bin/activate
pip install scrapy
pip install uwsgi
sudo nano /etc/systemd/system/uwsgi.service
sudo systemctl start uwsgi
git clone https://github.com/neoweb2212/ubuntu-scrapy-setup.git
nano ~/.ssh/config
git clone git@github.com:neoweb2212/ubuntu-scrapy-setup.git
cd ubuntu-scrapy-setup
ssh-add -l
eval "$(ssh-agent -s)"
ssh-add ~/.ssh/id_rsa
ls ~/.ssh/EmailFinderSshKey
EmailFinderSshKeyls ~/.ssh/EmailFinderSshKey
ls ~/.ssh/EmailFinderSshKey
nano ~/.ssh/EmailFinderSshKey
chmod 600 ~/.ssh/EmailFinderSshKey
nano ~/.ssh/EmailFinderSshKey.pub
ssh-keygen -t rsa -b 4096 -C "chvandendriessche@neomnia.net" -f ~/.ssh/EmailFinderSshKey
ssh-keygen -t rsa -b 4096 -C "chvandendriessche@neomnia.net" -f ~/.ssh/CustomUbuntuScrapySetting
cat ~/.ssh/CustomUbuntuScrapySetting.pub
nano ~/.ssh/config
ssh -T git@github.com
eval "$(ssh-agent -s)"
ssh-add ~/.ssh/CustomUbuntuScrapySetting
ssh-add -l
nano ~/.ssh/config
sh -T git@github.com
ssh -T git@github.com
cat ~/.ssh/CustomUbuntuScrapySetting.pub
eval "$(ssh-agent -s)"
ssh-add ~/.ssh/CustomUbuntuScrapySetting
eval "$(ssh-agent -s)"
ssh-add ~/.ssh/CustomUbuntuScrapySetting
nano ~/.ssh/config
ssh -T git@github.com
chmod 600 ~/.ssh/CustomUbuntuScrapySetting
ssh -v -T git@github.com
rm ~/.ssh/CustomUbuntuScrapySetting ~/.ssh/CustomUbuntuScrapySetting.pub
ssh-keygen -t rsa -b 4096 -C "chvandendriessche@neomnia.net" -f ~/.ssh/CustomUbuntuScrapySetting
cat ~/.ssh/CustomUbuntuScrapySetting.pub
nano ~/.ssh/config
ssh -T git@github.com
eval "$(ssh-agent -s)"
ssh-add ~/.ssh/CustomUbuntuScrapySetting
chmod 600 ~/.ssh/CustomUbuntuScrapySetting
chmod 644 ~/.ssh/CustomUbuntuScrapySetting.pub
nano ~/.ssh/config
ssh -v -T git@github.com
Host github.com
nano ~/.ssh/config
chmod 600 ~/.ssh/CustomUbuntuScrapySetting
chmod 644 ~/.ssh/CustomUbuntuScrapySetting.pub
eval "$(ssh-agent -s)"
ssh-add ~/.ssh/CustomUbuntuScrapySetting
ssh -vT git@github.com
cat ~/.ssh/CustomUbuntuScrapySetting.pub
ssh -vT git@github.com
git clone git@github.com:neoweb2212/ubuntu-scrapy-setup.git
rm -rf ubuntu-scrapy-setup
git clone git@github.com:neoweb2212/ubuntu-scrapy-setup.git
rm -rf ubuntu-scrapy-setup
cd ubuntu-scrapy-setup
git clone git@github.com:neoweb2212/ubuntu-scrapy-setup.git
cd ubuntu-scrapy-setup
rm -rf *
rm -rf .[^.] .??*
git add .
rm -rf ubuntu-scrapy-setup
git clone git@github.com:neoweb2212/ubuntu-scrapy-setup.git
cd ubuntu-scrapy-setup
rm -rf *
rm -rf .[^.] .??*
git add .
pwd
ls -a
rm -rf ubuntu-scrapy-setup
git clone git@github.com:neoweb2212/ubuntu-scrapy-setup.git
cd ubuntu-scrapy-setup
git clone git@github.com:neoweb2212/ubuntu-scrapy-setup.git
rm -rf ubuntu-scrapy-setup
git clone git@github.com:neoweb2212/ubuntu-scrapy-setup.git
cd ubuntu-scrapy-setup
git clone git@github.com:neoweb2212/ubuntu-scrapy-setup.git
cd ubuntu-scrapy-setup
rm -rf *
rm -rf .[^.] .??*
git add .
git push origin main
git clone git@github.com:neoweb2212/ubuntu-scrapy-setup.git
cd ubuntu-scrapy-setup
rm -rf *
rm -rf .[^.] .??*
git add .
pwd
ls -a
cd ~
rm -rf ubuntu-scrapy-setup
git clone git@github.com:neoweb2212/ubuntu-scrapy-setup.git
cd ubuntu-scrapy-setup
pwd
ls -a
rm -rf CNAME assets favicon.ico images index.html
ls -a
git add .
git commit -m "Supprimer tous les fichiers du dépôt"
git push origin main
mkdir ~/ubuntu-scrapy-config
cp /chemin/vers/fichier/de/config ~/ubuntu-scrapy-config/
mkdir ~/ubuntu-scrapy-config
ls -la ~/ubuntu-scrapy-config/
ls ~/.scrapy
ls ~/projects/
sudo find / -name "*.py" | grep -i scrapy
sudo find / -name "scrapy.cfg"
ls -la ~
ls -R ~/EmailFinder
ls -R ~/ubuntu-scrapy-setupcd ~/ubuntu-scrapy-setup
touch scrapy.cfg
mkdir project
touch project/__init__.py
touch project/settings.py
touch project/items.py
touch project/pipelines.py
touch project/middlewares.py
mkdir project/spiders
touch project/spiders/__init__.py
nano scrapy.cfg
nano echo "# Scrapy settings for project
BOT_NAME = 'project'
SPIDER_MODULES = ['project.spiders']
NEWSPIDER_MODULE = 'project.spiders'" > project/settings.py
git init
git add .
git branch -m main
git rm --cached ubuntu-scrapy-setup
git reset HEAD
tree
sudo snap instal tree
sudo apt install tree
tree
rm -r ubuntu-scrapy-config ubuntu-scrapy-setup
pwd*
pwd
sudo rm -rf ubuntu-scrapy-setup
ls -la
git init
echo "venv/" > .gitignore
echo "*.pyc" >> .gitignore
echo "__pycache__/" >> .gitignorels -la
ls -la
mv .gitignorels .gitignore
git add .
git status
git commit -m "Initial Scrapy project structure"
git remote add origin https://github.com/neoweb2212/ubuntu-scrapy-setup.git
git remote -v
git push -u origin main
git remote set-url origin git@github.com:neoweb2212/ubuntu-scrapy-setup.git
git remote -v
git push -u origin main
git push -u origin master
git add .
git commit -m "Initial commit"
git push -u origin HEAD:main
git push -u origin HEAD:master
git checkout -b main
git push -u origin main
tree
cd ubuntu
cd /home/ubuntu
tree
tree /chemin/vers/le/repertoire/ubuntu
tree /home/ubuntu
tree ~/ubuntu
tree /home/ubuntu
tree -L 2 /home/ubuntu
tree -L 3 /home/ubuntu
# Créer le dossier root pour zxz.fr
mkdir -p /home/ubuntu/root/nginx/sites-available
mkdir -p /home/ubuntu/root/uwsgi/apps-available
mkdir -p /home/ubuntu/root/config
# Créer les dossiers pour les projets existants
mkdir -p /home/ubuntu/emailfinder
mkdir -p /home/ubuntu/websitecrawl
nano /home/ubuntu/root/config/routing.py
nano /home/ubuntu/root/nginx/sites-available/zxz.fr
nano /home/ubuntu/root/uwsgi/apps-available/emailfinder.ini
nano /home/ubuntu/root/uwsgi/apps-available/websitecrawl.ini
server {
}nano server {
}sudo ln -s /home/ubuntu/root/nginx/sites-available/zxz.fr /etc/nginx/sites-enabled/
sudo nginx -t
sudo systemctl reload nginx
sudo apt update
sudo systemctl status nginx
sudo apt update
sudo apt install nginx
sudo systemctl status nginx
sudo ln -s /home/ubuntu/root/nginx/sites-available/zxz.fr /etc/nginx/sites-enabled/
sudo apt install uwsgi uwsgi-plugin-python3
sudo mkdir -p /etc/uwsgi/apps-enabled
nano /home/ubuntu/emailfinder/wsgi.py
cd /home/ubuntu/emailfinder
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
deactivate
cd /home/ubuntu/websitecrawl
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
cd /home/ubuntu/emailfinder
touch requirements.txt
nano requirements.txt
cd /home/ubuntu/websitecrawl
touch requirements.txt
nano requirements.txt
mkdir /home/ubuntu/backups
cp -r /home/ubuntu/emailfinder /home/ubuntu/backups/emailfinder_backup_$(date +%Y%m%d)
cp -r /home/ubuntu/websitecrawl /home/ubuntu/backups/websitecrawl_backup_$(date +%Y%m%d)cp -r /home/ubuntu/websitecrawl /home/ubuntu/backups/websitecrawl_backup_$(date +%Y%m%d)
ls -l /home/ubuntu/backups
echo "Dossier de sauvegarde pour les projets emailfinder et websitecrawl" > /home/ubuntu/backups/README.txt
echo "Créé le $(date)" >> /home/ubuntu/backups/README.txt
sudo chown -R ubuntu:ubuntu /home/ubuntu/backups
ls -l /home/ubuntu/websitecrawl
cp -r /home/ubuntu/websitecrawl /home/ubuntu/backups/websitecrawl_backup_$(date +%Y%m%d)
ls -l /home/ubuntu/backups
nano /home/ubuntu/backup_script.sh
chmod +x /home/ubuntu/backup_script.sh
crontab -e
cd /home/ubuntu/emailfinder
nano requirements.txt
scrapy==2.5.1
flask==2.0.1
uwsgi==2.0.20
nano requirements.txt
cd /home/ubuntu/emailfinder
source venv/bin/activate
pip install --upgrade pip
pip list
deactivate
Flask==2.0.1
Scrapy==2.5.1
uWSGI==2.0.20
nano /home/ubuntu/emailfinder/requirements.txt
source /home/ubuntu/emailfinder/venv/bin/activate
pip install -r requirements.txt
sudo apt-get update
pip install uWSGI==2.0.20
pip install uWSGI
sed -i 's/uWSGI==2.0.20/uWSGI==2.0.26/' requirements.txt
pip install -r requirements.txt
pip list
pip freeze > requirements.txt
deactivate
cd /home/ubuntu/websitecrawl
python3 -m venv venv
source venv/bin/activate
nano requirements.txt
pip list --outdated
pip install -r requirements.txt
pip list
scrapy startproject websitecrawl .
scrapy genspider example example.com
tree
tree /home/ubuntu
sudo apt-get update
tree /home/ubuntu
tree -L 2 /home/ubuntu
rm /home/ubuntu/scrapy.cfg
ls -la /home/ubuntu/project
tree -L2 /home/ubuntu
tree -L 2 /home/ubuntu
cd /home/ubuntu/websitecrawl
scrapy startproject websitecrawl .
scrapy genspider lapollo lapollo.fr
nano websitecrawl/spiders/lapollo.py
scrapy crawl lapollo -o lapollo_output.json
mkdir web_crawler_api
cd web_crawler_api
python3 -m venv venv
source venv/bin/activate
pip install flask celery scrapy
cd /home/ubuntu/websitecrawl
tree -L 2
cd /home/ubuntu/websitecrawl
touch extractors.py spider.py app.py
nano extractors.py
nano app.py
deactivate
cd /home/ubuntu/websitecrawl
mv venv ../venv
cd /home/ubuntu/websitecrawl
tree -L 2
nano app.py
rm -rf web_crawler_api
mv web_crawler_api/venv ./venv
tree -L2
source venv/bin/activate
tree
cd /home/ubuntu/websitecrawl
tree -L 2
ls ../venv
python3 -m venv ../venv
source ../venv/bin/activate
pip install -r requirements.txt
python app.py
curl -X POST -H "Content-Type: application/json" -d '{"domain": "example.com", "search_types": ["email", "price"]}' http://localhost:5000/crawl
deactivate
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
python3 app.py
source venv/bin/activate
python3 -m venv venv
source venv/bin/activate
pip install scrapy flask celery
pip list
cat app.py
cat websitecrawl/spiders/lapollo.py
nano app.py
nano extractors.py
rm .extractors.py.swp
rm ./.extractors.py.swp
ls -la
nano extractors.py
ls -lals -la
nano extractors.py
df -h
du -sh /* 2>/dev/null | sort -rh | head -n 10
ls -lh
cd ubuntu
cd /home/ubuntu/backups
ls -t | tail -n +2 | xargs rm -rf
cd /home/ubuntu/websitecrawl
cat extractors.py
nano extractors.py
cat app.py
python3 app.py
pip install flask celery scrapy redis
sudo systemctl status redis-server
cd ~
python3 -m venv my_venv
source my_venv/bin/activate
pip install flask celery scrapy redis
sudo apt update
source venv/bin/activate
celery -A app.celery worker --loglevel=info
curl -X POST -H "Content-Type: application/json" -d '{"domain": "lapollo.fr", "search_types": ["email", "price"]}' http://localhost:5000/crawl
cd ~
python3 -m venv my_venv
source my_venv/bin/activate
pip install flask celery scrapy redis
sudo apt update
sudo systemctl status redis-server
pip install flask celery scrapy redis
sudo apt update
sudo systemctl status redis-server
dpkg -l | grep redis
sudo apt update
sudo systemctl status redis-server
sudo apt install redis
sudo systemctl status redis-server
cd ~/websitecrawl
source ~/my_venv/bin/activate
python3 app.py
source ~/my_venv/bin/activate
cd ~/websitecrawl
celery -A app.celery worker --loglevel=info
curl -X POST -H "Content-Type: application/json" -d '{"domain": "example.com", "search_types": ["email", "price"]}' http://localhost:5000/crawl
source ~/websitecrawl_env/bin/activate
cd ~/websitecrawl
celery -A app.celery worker --loglevel=info
cd /home/ubuntu/websitecrawl
ls
python3 app.py
celery -A app.celery worker --loglevel=info
pwd
ls
pip list
cd /home/ubuntu/websitecrawl
source ../venv/bin/activate
pip list
pip install flask celery scrapy
deactivate
python3 -m venv ~/websitecrawl_env
source ~/websitecrawl_env/bin/activate
pip install flask celery scrapy
ls
python3 app.py
source ~/websitecrawl_env/bin/activate
cd ~/websitecrawl
python3 app.py
source ~/websitecrawl_env/bin/activate
cd ~/websitecrawl
celery -A app.celery worker --loglevel=info
curl -X POST -H "Content-Type: application/json" -d '{"domain": "example.com", "search_types": ["email", "price"]}' http://localhost:5000/crawl
source ~/websitecrawl_env/bin/activate
cd ~/websitecrawl
celery -A app.celery worker --loglevel=info
curl -X POST -H "Content-Type: application/json" -d '{"domain": "example.com", "search_types": ["email", "price"]}' http://172.31.35.188:5000/crawl
app.run(debug=True, host='0.0.0.0')
source ~/websitecrawl_env/bin/activate
cd ~/websitecrawl
celery -A app.celery worker --loglevel=info
curl -X POST -H "Content-Type: application/json" -d '{"domain": "example.com", "search_types": ["email", "price"]}' http://localhost:5000/crawl
nano app.py
cd ~/websitecrawl
ls -la
source ~/websitecrawl_env/bin/activate
pip list
cat app.py
app.run(debug=True, host='0.0.0.0')
nano app.py
python3 app.py
nano app.py
source ~/websitecrawl_env/bin/activate
cd ~/websitecrawl
celery -A app.celery worker --loglevel=info
nano app.py
pip install gunicorn
gunicorn --bind 0.0.0.0:5000 app:app
celery -A app.celery worker --loglevel=info
cd ~/websitecrawl
source ~/websitecrawl_env/bin/activate
celery -A websitecrawl.app.celery worker --loglevel=info
ls -R
celery -A websitecrawl.app.celery worker --loglevel=info
celery -A app.celery worker --loglevel=info
pip list | grep redis
pip install redis
sudo systemctl status redis-server
nano app.py
nano apps.py
nano app.py
nanpo app.py
nano app.py
python3 app.py
curl -X POST -H "Content-Type: application/json" -d '{"domain": "example.com", "search_types": ["email", "price"]}' http://localhost:5000/crawl
python3 app.py &
celery -A app.celery worker --loglevel=info &
curl -X POST -H "Content-Type: application/json" -d '{"domain": "example.com", "search_types": ["email", "price"]}' http://localhost:5000/crawl
source ~/websitecrawl_env/bin/activate
curl -X POST -H "Content-Type: application/json" -d '{"domain": "example.com", "search_types": ["email", "price"]}' http://localhost:5000/crawl
curl http://localhost:5000
curl -X POST -H "Content-Type: application/json" -d '{"domain": "example.com", "search_types": ["email", "price"]}' http://172.31.35.188:5000/crawl
nano ~ app.py
cd ubuntu
dir ubuntu
cd ubuntu
cd /home/ubuntu
ls
cd websitecrawl
nano app.py
celery = Celery('app', broker='redis://localhost:6379/0', backend='redis://localhost:6379/0')
ubuntu@ip-172-31-35-188:~/websitecrawl$ celery = Celery('app', broker='redis://localhost:6379/0', backend='redis://localhost:6379/0')
-bash: syntax error near unexpected token `('
nano app.py
python3 app.py &
pwd
ls -R
cat websitecrawl/__init__.py
source ../websitecrawl_env/bin/activate
pip install flask celery scrapy redis
pip list
python3 app.py
source ../websitecrawl_env/bin/activate
pip install flask celery scrapy redis
pip list
python3 app.py
pip install -r requirements.txt
cd ~/websitecrawl
source ../websitecrawl_env/bin/activate
celery -A app.celery worker --loglevel=info
nano app.py
cd ubuntu
cd ~/websitecrawl
source ../websitecrawl_env/bin/activate
python3 app.py
nano app.py
python3 app.py
sudo python3 app.py
curl http://13.39.17.60:5000/test
cd ~/websitecrawl
source ../websitecrawl_env/bin/activate
python3 app.py
cd ~/websitecrawl
python3 app.py
cd ~/websitecrawl
source ../websitecrawl_env/bin/activate
ps aux | grep python
cd ~/websitecrawl
source ../websitecrawl_env/bin/activate
python3 app.py
nano app.py
python3 app.py
cd ~/websitecrawl
source ../websitecrawl_env/bin/activate
pip install flask celery scrapy redis
cd ~/websitecrawl
source ../websitecrawl_env/bin/activate
cat app.py
python3 app.py
curl http://localhost:5000/test
ps aux | grep python
python3 app.py
cd ~/websitecrawl
source ../websitecrawl_env/bin/activate
curl http://localhost:5000/test
curl http://127.0.0.1:5000/test
cd ~/ubuntu/backups
ls
cd ubuntu
cd home/ubuntu
cd /home/ubuntu
